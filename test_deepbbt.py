"""
Given trained prompt and corresponding template, we ask test API for predictions
Shallow version
We train one prompt after embedding layer, so we use sentence_fn and embedding_and_attention_mask_fn.
Baseline code is in bbt.py
"""


import os
import torch
from transformers import RobertaConfig, RobertaTokenizer
from models.modeling_roberta import RobertaModel
import numpy as np
import csv
import argparse
from tfidf import generate_incontext_use_tfidf

parser = argparse.ArgumentParser()
parser.add_argument("--task_name", default='SNLI', type=str, help='[SST-2, Yelp, AGNews, TREC, MRPC, SNLI]')
# parser.add_argument("--seed", default=8, type=int)
parser.add_argument("--device", default='cuda:3', type=str)
parser.add_argument("--offline", default=True, action='store_true', help='for transformers offline')
parser.add_argument("--instruction", default=False, action='store_true', help='for instruction learning')
parser.add_argument("--multiVerbalizer", default=False, action='store_true', help='for TREC')
parser.add_argument("--use_tfidf", default=False, action='store_true', help="use tfidf to generate in-contexts.")
parser.add_argument("--use_rlprompt", default=False, action='store_true', help="use hard prompt generated by rlprompt.")
parser.add_argument("--results_dir", default='GDA_neg/last_epoch_results', type=str)


args = parser.parse_args()
if args.offline:
    from test_api_custom_model_path import test_api
    from test_api_custom_model_path import RobertaEmbeddings
    model_name = 'transformer_model/roberta-large'
else:
    from test_api import test_api
    from test_api import RobertaEmbeddings
    model_name = 'roberta-large'


tokenizer = RobertaTokenizer.from_pretrained(model_name)
results_dir = args.results_dir

print(args)
# print('task name:', args.task_name, 'seed:', args.seed)


INSTRUCTION_DICT = {
    'SST-2': '. Your task is to classify the movie review as "bad" or "great" based on its content.',
    'Yelp': '. Your task is to classify the review as "bad" or "great" based on its content.',
    # 'AGNews': 'Your task is to classify the news as world" or sports or business or technology based on its content . ',
    'AGNews': "World news : Party of Brazil's President Party Stronger In its first electoral test since taking power 21 months ago, the party of Brazil's left-leaning president emerged stronger from nationwide municipal elections but could not come in first in the country's biggest and most important city, Sao Paulo . Technology news : Microsoft Warns Asian Governments of Linux Suits Microsoft Corp. (MSFT.O: Quote, Profile, Research) warned Asian governments on Thursday they could face patent lawsuits for using the Linux operating system instead of its Windows software . Business news : US Sues Sears, Accusing It of Racial Bias The Equal Employment Opportunity Commission has sued Sears, Roebuck, contending that it illegally fired an automotive repair store manager because he was black . Sports news : Keenan McCardell Could Start for Chargers (AP) AP - Newly acquired wide receiver Keenan McCardell will make his season debut on Sunday and might even start for the San Diego Chargers in their road game against the Carolina Panthers . ",
    'MRPC': 'Your task is to judge the entailment relationship of two news as No or Yes based on their content . ',
    'SNLI': 'Your task is to judge the entailment relationship of two sentences as Yes, Maybe, No based on their content . ',
    'TREC': 'Your task is to classify the problems as description or entity or abbreviation or human or numeric or location based on its content . ',
}

IN_CONTEXT = {
    'SST-2': '',
    'Yelp': '',
    'AGNews': 'Technology News : Election apology starts net feud A website that apologises to the world for the US election results has been hugely successful. \
               Sports News : NBA Today . The Suns (18-3) have the NBA\'s best record and have won nine of their last 10 games. ',
    'MRPC': 'For example , None of Deans opponents picked him as someone to party with , nor was Dean asked that question . ? Yes , None of Dean \'s opponents picked him as someone to party with and Dean was not asked the question . \
             I loved the Brazilian music I played . ? No , " I \'ve played Brazilian music , but I \'m not Brazilian . ',
    'SNLI': 'For example , Girl in plaid shirt riding a unicycle. ? Yes , A girl is riding. a woman sits on the rock. ? No , A woman is riding her bicycle. Bunch of people celebrating a holiday. ? Maybe , There is a cake with candles.',
    'TREC': 'Entity question : What is a fear of bees ? Numeric question : What is Dick Clark \'s birthday ? Abbreviation question : What does BUD stand for ? ',
}

def prepad_prompt(instruction, n_prompt_tokens, tokenizer):
    ins_id = tokenizer.encode(instruction, add_special_tokens=False)
    print("the length of instruction + in-context is " + str(len(ins_id)))
    if len(ins_id) < 50:
        ran_id = list(range(1000, 1000 + n_prompt_tokens - len(ins_id)))
        prompt = tokenizer.decode(ran_id + ins_id)
    else:
        ins_id = ins_id[:50]
        prompt = tokenizer.decode(ins_id)
    return prompt

def sentence_fn_factory(task_name, seed):
    if args.instruction:
        instruction = INSTRUCTION_DICT[task_name]
        if args.use_tfidf:
            train_data_path = './datasets/{}/{}/train.tsv'.format(args.task_name, seed)
            in_contexts = generate_incontext_use_tfidf(train_data_path)
        else:
            in_contexts = IN_CONTEXT[task_name]
        prompt_initialization = prepad_prompt(instruction=instruction+in_contexts, n_prompt_tokens=50, tokenizer=tokenizer)
    else:
        offset = 1000
        prompt_initialization = tokenizer.decode(list(range(offset, offset + 50)))
    if task_name == 'MRPC':
        def sentence_fn(test_data):
            return prompt_initialization + ' . ' + test_data + ' ? <mask> , ' + test_data

    elif task_name == 'SNLI':
        if args.use_rlprompt:
            if seed == 8:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . ' + test_data + ' UpgradeVariableServiceStackStatus <mask> ' + test_data
            elif seed == 13:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . ' + test_data + ' MemoryListenerJSONJSONJSON <mask> ' + test_data
            elif seed == 42:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . ' + test_data + ' ServerFolderResourceByIdHardware <mask> ' + test_data
            elif seed == 50:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . ' + test_data + ' DirectoryDirectoryDirectoryDirectoryDirectory <mask> ' + test_data
            elif seed == 60:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . ' + test_data + ' FramesContextValueConnectionArgs <mask> ' + test_data
        else:
            def sentence_fn(test_data):
                return prompt_initialization + ' . ' + test_data + ' ? <mask> , ' + test_data
    elif task_name == 'SST-2':
        def sentence_fn(test_data):
            return prompt_initialization + ' . ' + test_data + ' . It was <mask> .'

    elif task_name == 'AGNews':
        if args.use_rlprompt:
            if seed == 8:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> ResearchMemberEmployCouncilOrgan ' + test_data
            elif seed == 13:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> ReviewMonitorDesignReportReport ' + test_data
            elif seed == 42:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> ReviewOfficialChoiceLatestNews ' + test_data
            elif seed == 50:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> ReviewPanelScopeCategorySort ' + test_data
            elif seed == 60:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> GearInformationSocialResponseResources ' + test_data
        else:
            def sentence_fn(test_data):
                return prompt_initialization + ' . <mask> News: ' + test_data

    elif task_name == 'TREC':
        if args.use_rlprompt:
            if seed == 8:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> DefenseMaterialInfoMovieProject ' + test_data
            elif seed == 13:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> ResultEventBrainQueryBattery ' + test_data
            elif seed == 42:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> HelperRoamingAdapterGridMsg ' + test_data
            elif seed == 50:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> DriverIntegerBenchComputerHandler ' + test_data
            elif seed == 60:
                def sentence_fn(test_data):
                    return prompt_initialization + ' . <mask> DistanceEventArgsWriterNode ' + test_data
        else:
            def sentence_fn(test_data):
                return prompt_initialization + ' . <mask> question:' + test_data

    elif task_name == 'Yelp':
        def sentence_fn(test_data):
            return prompt_initialization + ' . ' + test_data + ' .It was <mask> .'
    else:
        raise ValueError

    return sentence_fn

verbalizer_dict = {
    'SNLI': ["Yes", "Maybe", "No"],
    'MRPC': ["No", "Yes"],
    'SST-2': ["bad", "great"],
    'AGNews': ["World", "Sports", "Business", "Technology"],
    'TREC': ["Description", "Entity", "Abbreviation", "Human", "Numeric", "Location"],
    'Yelp': ["bad", "great"]
}


device = args.device
print('device', device)

for task_name in [args.task_name]:  # 'SNLI', 'SST-2', 'MRPC', 'AGNews', 'TREC', 
    for seed in [8, 13, 42, 50, 60]:  # 8, 13, 42, 50, 60
        torch.manual_seed(seed)
        np.random.seed(seed)
        # CM = torch.load(f'./bbtv2_results/{task_name}/{seed}/CM.pt').to(device)
        best_prompt = torch.load(f'./{results_dir}/{task_name}/{seed}/best.pt').to(device).view(24, 50, -1)

        sentence_fn = sentence_fn_factory(task_name, seed)
        # def embedding_and_attention_mask_fn(embedding, attention_mask):
        #     # res = torch.cat([init_prompt[:-5, :], input_embed, init_prompt[-5:, :]], dim=0)
        #     prepad = torch.zeros(size=(1, 1024), device=device)
        #     pospad = torch.zeros(size=(embedding.size(1) - 51, 1024), device=device)
        #     return embedding + torch.cat([prepad, best, pospad]), attention_mask

        def hidden_states_and_attention_mask_fn(i, embedding, attention_mask):
            prepad = torch.zeros(size=(1, 1024), device=device)
            pospad = torch.zeros(size=(embedding.size(1) - 51, 1024), device=device)
            return embedding + torch.cat([prepad, best_prompt[i], pospad]), attention_mask

        predictions = torch.tensor([], device=device)

        if args.offline:
            for res, _, _ in test_api(
                sentence_fn=sentence_fn,
                # embedding_and_attention_mask_fn=embedding_and_attention_mask_fn,
                hidden_states_and_attention_mask_fn=hidden_states_and_attention_mask_fn,
                test_data_path=f'./test_datasets/{task_name}/encrypted.pth',
                task_name=task_name,
                device=device,
                tokenizer_path=model_name, 
                model_path=model_name,
            ):
                if args.multiVerbalizer:
                    if args.task_name == 'TREC':
                        # c0 = res[:, tokenizer.encode("Description", add_special_tokens=False)[0]]
                        c01 = res[:, tokenizer.encode("Definition", add_special_tokens=False)[0]]
                        c02 = res[:, tokenizer.encode("Description", add_special_tokens=False)[0]]
                        c03 = res[:, tokenizer.encode("Manner", add_special_tokens=False)[0]]
                        c04 = res[:, tokenizer.encode("Reason", add_special_tokens=False)[0]]
                        c0 = torch.stack([c01, c02, c03, c04]).mean(dim=0)

                        # c1 = res[:, tokenizer.encode("Entity", add_special_tokens=False)[0]]
                        c11 = res[:, tokenizer.encode("Animal", add_special_tokens=False)[0]]
                        c12 = res[:, tokenizer.encode("Body", add_special_tokens=False)[0]]
                        c13 = res[:, tokenizer.encode("Color", add_special_tokens=False)[0]]
                        c14 = res[:, tokenizer.encode("Creative", add_special_tokens=False)[0]]
                        c15 = res[:, tokenizer.encode("Currency", add_special_tokens=False)[0]]
                        c16 = res[:, tokenizer.encode("Diseases", add_special_tokens=False)[0]]
                        c17 = res[:, tokenizer.encode("Medicine", add_special_tokens=False)[0]]
                        c18 = res[:, tokenizer.encode("Event", add_special_tokens=False)[0]]
                        c19 = res[:, tokenizer.encode("Food", add_special_tokens=False)[0]]
                        c110 = res[:, tokenizer.encode("Instrument", add_special_tokens=False)[0]]
                        c111 = res[:, tokenizer.encode("Lang", add_special_tokens=False)[0]]
                        c112 = res[:, tokenizer.encode("Letter", add_special_tokens=False)[0]]
                        c113 = res[:, tokenizer.encode("Entity", add_special_tokens=False)[0]]
                        c114 = res[:, tokenizer.encode("Plant", add_special_tokens=False)[0]]
                        c115 = res[:, tokenizer.encode("Product", add_special_tokens=False)[0]]
                        c116 = res[:, tokenizer.encode("Religion", add_special_tokens=False)[0]]
                        c117 = res[:, tokenizer.encode("Sport", add_special_tokens=False)[0]]
                        c118 = res[:, tokenizer.encode("Substance", add_special_tokens=False)[0]]
                        c119 = res[:, tokenizer.encode("Symbol", add_special_tokens=False)[0]]
                        c120 = res[:, tokenizer.encode("Technique", add_special_tokens=False)[0]]
                        c121 = res[:, tokenizer.encode("Term", add_special_tokens=False)[0]]
                        c122 = res[:, tokenizer.encode("Vehicle", add_special_tokens=False)[0]]
                        c123 = res[:, tokenizer.encode("Word", add_special_tokens=False)[0]]
                        c1 = torch.stack([c11, c12, c13, c14, c15, c16, c17, c18, c19, c110, c111, c112, c113, c114, c115, c116, c117, c118, c119, c120, c121, c122, c123]).mean(dim=0)
                        
                        # c2 = res[:, tokenizer.encode("Abbreviation", add_special_tokens=False)[0]]
                        c21 = res[:, tokenizer.encode("Abbreviation", add_special_tokens=False)[0]]
                        c22 = res[:, tokenizer.encode("Expression", add_special_tokens=False)[0]]
                        c2 = torch.stack([c21, c22]).mean(dim=0)

                        # c3 = res[:, tokenizer.encode("Human", add_special_tokens=False)[0]]
                        c31 = res[:, tokenizer.encode("Group", add_special_tokens=False)[0]]
                        c32 = res[:, tokenizer.encode("Organization", add_special_tokens=False)[0]]
                        c33 = res[:, tokenizer.encode("Individual", add_special_tokens=False)[0]]
                        c34 = res[:, tokenizer.encode("Title", add_special_tokens=False)[0]]
                        c35 = res[:, tokenizer.encode("Person", add_special_tokens=False)[0]]
                        c36 = res[:, tokenizer.encode("Human", add_special_tokens=False)[0]]
                        c3 = torch.stack([c31, c32, c33, c34, c35, c36]).mean(dim=0)

                        # c4 = res[:, tokenizer.encode("Numeric", add_special_tokens=False)[0]]
                        c41 = res[:, tokenizer.encode("Code", add_special_tokens=False)[0]]
                        c42 = res[:, tokenizer.encode("Count", add_special_tokens=False)[0]]
                        c43 = res[:, tokenizer.encode("Date", add_special_tokens=False)[0]]
                        c44 = res[:, tokenizer.encode("Distance", add_special_tokens=False)[0]]
                        c45 = res[:, tokenizer.encode("Money", add_special_tokens=False)[0]]
                        c46 = res[:, tokenizer.encode("Order", add_special_tokens=False)[0]]
                        c47 = res[:, tokenizer.encode("Number", add_special_tokens=False)[0]]
                        c48 = res[:, tokenizer.encode("Period", add_special_tokens=False)[0]]
                        c49 = res[:, tokenizer.encode("Percent", add_special_tokens=False)[0]]
                        c410 = res[:, tokenizer.encode("Speed", add_special_tokens=False)[0]]
                        c411 = res[:, tokenizer.encode("Temperature", add_special_tokens=False)[0]]
                        c412 = res[:, tokenizer.encode("Size", add_special_tokens=False)[0]]
                        c413 = res[:, tokenizer.encode("Weight", add_special_tokens=False)[0]]
                        c414 = res[:, tokenizer.encode("Area", add_special_tokens=False)[0]]
                        c415 = res[:, tokenizer.encode("Volume", add_special_tokens=False)[0]]
                        c4 = torch.stack([c41, c42, c43, c44, c45, c46, c47, c48, c49, c410, c411, c412, c413, c414, c415]).mean(dim=0)

                        # c5 = res[:, tokenizer.encode("Location", add_special_tokens=False)[0]]
                        c51 = res[:, tokenizer.encode("City", add_special_tokens=False)[0]]
                        c52 = res[:, tokenizer.encode("Country", add_special_tokens=False)[0]]
                        c53 = res[:, tokenizer.encode("Mountain", add_special_tokens=False)[0]]
                        c54 = res[:, tokenizer.encode("Location", add_special_tokens=False)[0]]
                        c55 = res[:, tokenizer.encode("State", add_special_tokens=False)[0]]
                        c5 = torch.stack([c51, c52, c53, c54, c55]).mean(dim=0)
                        pred = torch.stack([c0, c1, c2, c3, c4, c5]).argmax(dim=0)
                        predictions = torch.cat([predictions, pred])
                    elif args.task_name == 'AGNews':
                        # c0 = res[:, tokenizer.encode("Description", add_special_tokens=False)[0]]
                        c01 = res[:, tokenizer.encode("World", add_special_tokens=False)[0]]
                        c02 = res[:, tokenizer.encode("Iran", add_special_tokens=False)[0]]
                        c03 = res[:, tokenizer.encode("Iraq", add_special_tokens=False)[0]]
                        c04 = res[:, tokenizer.encode("Regional", add_special_tokens=False)[0]]
                        c05 = res[:, tokenizer.encode("BuzzFeed", add_special_tokens=False)[0]]
                        c06 = res[:, tokenizer.encode("PM", add_special_tokens=False)[0]]
                        c0 = torch.stack([c01, c02, c03, c04, c05, c06]).mean(dim=0)

                        # c1 = res[:, tokenizer.encode("Entity", add_special_tokens=False)[0]]
                        c11 = res[:, tokenizer.encode("Sports", add_special_tokens=False)[0]]
                        c12 = res[:, tokenizer.encode("Athletics", add_special_tokens=False)[0]]
                        c13 = res[:, tokenizer.encode("Hockey", add_special_tokens=False)[0]]
                        c14 = res[:, tokenizer.encode("Sporting", add_special_tokens=False)[0]]
                        c15 = res[:, tokenizer.encode("Basketball", add_special_tokens=False)[0]]
                        c16 = res[:, tokenizer.encode("TEAM", add_special_tokens=False)[0]]
                        c1 = torch.stack([c11, c12, c13, c14, c15, c16]).mean(dim=0)
                        
                        # c2 = res[:, tokenizer.encode("Abbreviation", add_special_tokens=False)[0]]
                        c21 = res[:, tokenizer.encode("Business", add_special_tokens=False)[0]]
                        c22 = res[:, tokenizer.encode("Investment", add_special_tokens=False)[0]]
                        c23 = res[:, tokenizer.encode("Trade", add_special_tokens=False)[0]]
                        c24 = res[:, tokenizer.encode("Industry", add_special_tokens=False)[0]]
                        c25 = res[:, tokenizer.encode("ET", add_special_tokens=False)[0]]
                        c26 = res[:, tokenizer.encode("business", add_special_tokens=False)[0]]
                        c2 = torch.stack([c21, c22, c23, c24, c25, c26]).mean(dim=0)

                        # c3 = res[:, tokenizer.encode("Human", add_special_tokens=False)[0]]
                        c31 = res[:, tokenizer.encode("Technology", add_special_tokens=False)[0]]
                        c32 = res[:, tokenizer.encode("tech", add_special_tokens=False)[0]]
                        c33 = res[:, tokenizer.encode("technology", add_special_tokens=False)[0]]
                        c34 = res[:, tokenizer.encode("Tech", add_special_tokens=False)[0]]
                        c35 = res[:, tokenizer.encode("Mobile", add_special_tokens=False)[0]]
                        c36 = res[:, tokenizer.encode("Wired", add_special_tokens=False)[0]]
                        c3 = torch.stack([c31, c32, c33, c34, c35, c36]).mean(dim=0)

                        pred = torch.stack([c0, c1, c2, c3]).argmax(dim=0)
                        predictions = torch.cat([predictions, pred])
                    elif args.task_name == 'MRPC':
                        # c0 = res[:, tokenizer.encode("Description", add_special_tokens=False)[0]]
                        c01 = res[:, tokenizer.encode("No", add_special_tokens=False)[0]]
                        c02 = res[:, tokenizer.encode("Still", add_special_tokens=False)[0]]
                        c03 = res[:, tokenizer.encode("meanwhile", add_special_tokens=False)[0]]
                        c04 = res[:, tokenizer.encode("Finally", add_special_tokens=False)[0]]
                        c05 = res[:, tokenizer.encode("Actually", add_special_tokens=False)[0]]
                        c06 = res[:, tokenizer.encode("Later", add_special_tokens=False)[0]]
                        c0 = torch.stack([c01, c02, c03, c04, c05, c06]).mean(dim=0)

                        # c1 = res[:, tokenizer.encode("Entity", add_special_tokens=False)[0]]
                        c11 = res[:, tokenizer.encode("Yes", add_special_tokens=False)[0]]
                        c12 = res[:, tokenizer.encode("Furthermore", add_special_tokens=False)[0]]
                        c13 = res[:, tokenizer.encode("Thankfully", add_special_tokens=False)[0]]
                        c14 = res[:, tokenizer.encode("Instead", add_special_tokens=False)[0]]
                        c15 = res[:, tokenizer.encode("Rather", add_special_tokens=False)[0]]
                        c16 = res[:, tokenizer.encode("The", add_special_tokens=False)[0]]
                        c1 = torch.stack([c11, c12, c13, c14, c15, c16]).mean(dim=0)

                        pred = torch.stack([c0, c1]).argmax(dim=0)
                        predictions = torch.cat([predictions, pred])
                    elif args.task_name == 'SNLI':
                        c01 = res[:, tokenizer.encode("Seriously", add_special_tokens=False)[0]]
                        c02 = res[:, tokenizer.encode("Finally", add_special_tokens=False)[0]]
                        c03 = res[:, tokenizer.encode("YES", add_special_tokens=False)[0]]
                        c04 = res[:, tokenizer.encode("Yeah", add_special_tokens=False)[0]]
                        c05 = res[:, tokenizer.encode("Exactly", add_special_tokens=False)[0]]
                        c0 = torch.stack([c01, c02, c03, c04, c05]).mean(dim=0)

                        # c1 = res[:, tokenizer.encode("Entity", add_special_tokens=False)[0]]
                        c11 = res[:, tokenizer.encode("Now", add_special_tokens=False)[0]]
                        c12 = res[:, tokenizer.encode("This", add_special_tokens=False)[0]]
                        c13 = res[:, tokenizer.encode("Likely", add_special_tokens=False)[0]]
                        c14 = res[:, tokenizer.encode("Apparently", add_special_tokens=False)[0]]
                        c15 = res[:, tokenizer.encode("Suddenly", add_special_tokens=False)[0]]
                        c1 = torch.stack([c11, c12, c13, c14, c15]).mean(dim=0)
                        
                        # c2 = res[:, tokenizer.encode("Abbreviation", add_special_tokens=False)[0]]
                        c21 = res[:, tokenizer.encode("Nah", add_special_tokens=False)[0]]
                        c22 = res[:, tokenizer.encode("Next", add_special_tokens=False)[0]]
                        c23 = res[:, tokenizer.encode("Alas", add_special_tokens=False)[0]]
                        c24 = res[:, tokenizer.encode("Nope", add_special_tokens=False)[0]]
                        c25 = res[:, tokenizer.encode("Later", add_special_tokens=False)[0]]
                        c2 = torch.stack([c21, c22, c23, c24, c25]).mean(dim=0)

                        pred = torch.stack([c0, c1, c2]).argmax(dim=0)
                        predictions = torch.cat([predictions, pred])
                    else:
                        raise NotImplementedError
                    
                else:
                    verbalizers = verbalizer_dict[task_name]
                    intrested_logits = [res[:, tokenizer.encode(verbalizer, add_special_tokens=False)[0]] for verbalizer in verbalizers]
                    intrested_logits = torch.softmax(torch.stack(intrested_logits).T, dim=1)
                    # intrested_logits = torch.mul(intrested_logits, CM)
                    # if args.use_CM:
                    # for i in range(len(intrested_logits)):
                    #     intrested_logits[i] *= CM[0][i]
                    pred = intrested_logits.argmax(dim=1)
                    predictions = torch.cat([predictions, pred])
        else:
            for res, _, _ in test_api(
                sentence_fn=sentence_fn,
                # embedding_and_attention_mask_fn=embedding_and_attention_mask_fn,
                hidden_states_and_attention_mask_fn=hidden_states_and_attention_mask_fn,
                test_data_path=f'./test_datasets/{task_name}/encrypted.pth',
                task_name=task_name,
                device=device,
            ):
                if args.multiVerbalizer and args.task_name == 'TREC':
                    # c0 = res[:, tokenizer.encode("Description", add_special_tokens=False)[0]]
                    c01 = res[:, tokenizer.encode("Definition", add_special_tokens=False)[0]]
                    c02 = res[:, tokenizer.encode("Description", add_special_tokens=False)[0]]
                    c03 = res[:, tokenizer.encode("Manner", add_special_tokens=False)[0]]
                    c04 = res[:, tokenizer.encode("Reason", add_special_tokens=False)[0]]
                    c0 = torch.stack([c01, c02, c03, c04]).max(dim=0)[0]

                    # c1 = res[:, tokenizer.encode("Entity", add_special_tokens=False)[0]]
                    c11 = res[:, tokenizer.encode("Animal", add_special_tokens=False)[0]]
                    c12 = res[:, tokenizer.encode("Body", add_special_tokens=False)[0]]
                    c13 = res[:, tokenizer.encode("Color", add_special_tokens=False)[0]]
                    c14 = res[:, tokenizer.encode("Creative", add_special_tokens=False)[0]]
                    c15 = res[:, tokenizer.encode("Currency", add_special_tokens=False)[0]]
                    c16 = res[:, tokenizer.encode("Diseases", add_special_tokens=False)[0]]
                    c17 = res[:, tokenizer.encode("Medicine", add_special_tokens=False)[0]]
                    c18 = res[:, tokenizer.encode("Event", add_special_tokens=False)[0]]
                    c19 = res[:, tokenizer.encode("Food", add_special_tokens=False)[0]]
                    c110 = res[:, tokenizer.encode("Instrument", add_special_tokens=False)[0]]
                    c111 = res[:, tokenizer.encode("Lang", add_special_tokens=False)[0]]
                    c112 = res[:, tokenizer.encode("Letter", add_special_tokens=False)[0]]
                    c113 = res[:, tokenizer.encode("Entity", add_special_tokens=False)[0]]
                    c114 = res[:, tokenizer.encode("Plant", add_special_tokens=False)[0]]
                    c115 = res[:, tokenizer.encode("Product", add_special_tokens=False)[0]]
                    c116 = res[:, tokenizer.encode("Religion", add_special_tokens=False)[0]]
                    c117 = res[:, tokenizer.encode("Sport", add_special_tokens=False)[0]]
                    c118 = res[:, tokenizer.encode("Substance", add_special_tokens=False)[0]]
                    c119 = res[:, tokenizer.encode("Symbol", add_special_tokens=False)[0]]
                    c120 = res[:, tokenizer.encode("Technique", add_special_tokens=False)[0]]
                    c121 = res[:, tokenizer.encode("Term", add_special_tokens=False)[0]]
                    c122 = res[:, tokenizer.encode("Vehicle", add_special_tokens=False)[0]]
                    c123 = res[:, tokenizer.encode("Word", add_special_tokens=False)[0]]
                    c1 = torch.stack([c11, c12, c13, c14, c15, c16, c17, c18, c19, c110, c111, c112, c113, c114, c115, c116, c117, c118, c119, c120, c121, c122, c123]).max(dim=0)[0]
                    
                    # c2 = res[:, tokenizer.encode("Abbreviation", add_special_tokens=False)[0]]
                    c21 = res[:, tokenizer.encode("Abbreviation", add_special_tokens=False)[0]]
                    c22 = res[:, tokenizer.encode("Expression", add_special_tokens=False)[0]]
                    c2 = torch.stack([c21, c22]).max(dim=0)[0]

                    # c3 = res[:, tokenizer.encode("Human", add_special_tokens=False)[0]]
                    c31 = res[:, tokenizer.encode("Group", add_special_tokens=False)[0]]
                    c32 = res[:, tokenizer.encode("Organization", add_special_tokens=False)[0]]
                    c33 = res[:, tokenizer.encode("Individual", add_special_tokens=False)[0]]
                    c34 = res[:, tokenizer.encode("Title", add_special_tokens=False)[0]]
                    c35 = res[:, tokenizer.encode("Person", add_special_tokens=False)[0]]
                    c36 = res[:, tokenizer.encode("Human", add_special_tokens=False)[0]]
                    c3 = torch.stack([c31, c32, c33, c34, c35, c36]).max(dim=0)[0]

                    # c4 = res[:, tokenizer.encode("Numeric", add_special_tokens=False)[0]]
                    c41 = res[:, tokenizer.encode("Code", add_special_tokens=False)[0]]
                    c42 = res[:, tokenizer.encode("Count", add_special_tokens=False)[0]]
                    c43 = res[:, tokenizer.encode("Date", add_special_tokens=False)[0]]
                    c44 = res[:, tokenizer.encode("Distance", add_special_tokens=False)[0]]
                    c45 = res[:, tokenizer.encode("Money", add_special_tokens=False)[0]]
                    c46 = res[:, tokenizer.encode("Order", add_special_tokens=False)[0]]
                    c47 = res[:, tokenizer.encode("Number", add_special_tokens=False)[0]]
                    c48 = res[:, tokenizer.encode("Period", add_special_tokens=False)[0]]
                    c49 = res[:, tokenizer.encode("Percent", add_special_tokens=False)[0]]
                    c410 = res[:, tokenizer.encode("Speed", add_special_tokens=False)[0]]
                    c411 = res[:, tokenizer.encode("Temperature", add_special_tokens=False)[0]]
                    c412 = res[:, tokenizer.encode("Size", add_special_tokens=False)[0]]
                    c413 = res[:, tokenizer.encode("Weight", add_special_tokens=False)[0]]
                    c414 = res[:, tokenizer.encode("Area", add_special_tokens=False)[0]]
                    c415 = res[:, tokenizer.encode("Volume", add_special_tokens=False)[0]]
                    c4 = torch.stack([c41, c42, c43, c44, c45, c46, c47, c48, c49, c410, c411, c412, c413, c414, c415]).max(dim=0)[0]

                    # c5 = res[:, tokenizer.encode("Location", add_special_tokens=False)[0]]
                    c51 = res[:, tokenizer.encode("City", add_special_tokens=False)[0]]
                    c52 = res[:, tokenizer.encode("Country", add_special_tokens=False)[0]]
                    c53 = res[:, tokenizer.encode("Mountain", add_special_tokens=False)[0]]
                    c54 = res[:, tokenizer.encode("Location", add_special_tokens=False)[0]]
                    c55 = res[:, tokenizer.encode("State", add_special_tokens=False)[0]]
                    c5 = torch.stack([c51, c52, c53, c54, c55]).max(dim=0)[0]

                    pred = torch.stack([c0, c1, c2, c3, c4, c5]).argmax(dim=0)
                    predictions = torch.cat([predictions, pred])
                else:
                    verbalizers = verbalizer_dict[task_name]
                    intrested_logits = [res[:, tokenizer.encode(verbalizer, add_special_tokens=False)[0]] for verbalizer in verbalizers]
                    intrested_logits = torch.softmax(torch.stack(intrested_logits).T, dim=1)
                    # intrested_logits = torch.mul(intrested_logits, CM)
                    # if args.use_CM:
                    # for i in range(len(intrested_logits)):
                    #     intrested_logits[i] *= CM[0][i]
                    pred = intrested_logits.argmax(dim=1)
                    predictions = torch.cat([predictions, pred])

        if not os.path.exists(f'./predictions/{args.results_dir}/{task_name}'):
            os.makedirs(f'./predictions/{args.results_dir}/{task_name}')
        with open(f'./predictions/{args.results_dir}/{task_name}/{seed}.csv', 'w+') as f:
            wt = csv.writer(f)
            wt.writerow(['', 'pred'])
            wt.writerows(torch.stack([torch.arange(predictions.size(0)), predictions.detach().cpu()]).long().T.numpy().tolist())